{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Correción de Elipsis - Natural Language Processing**\n",
    "\n",
    "\n",
    "-----------------------\n",
    "<div align=\"right\">\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **I.- Introducción**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El preprocesamiento de textos es una etapa crucial en el Procesamiento de Lenguaje Natural (NLP), especialmente cuando se trabaja con transcripciones literales de entrevistas o focus groups, donde es común encontrar elipsis (o puntos suspensivos) que representan interrupciones, titubeos o correcciones del hablante, dificultando el análisis del texto si no se manejan adecuadamente.\n",
    "\n",
    "El objetivo de este jupyter notebook es presentar una solución en Python para procesar textos y corregir las elipsis, mejorando así la calidad del texto para análisis posteriores. Se abordarán tres casos principales: la eliminación de palabras y bigramas repetidos, el ajuste de artículos definidos e indefinidos y la corrección de patrones gramaticales específicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II.- Desarrollo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para corregir el uso de elipsis en transcripciones literales típicas de entrevistas y focus groups, re realizarán varios pasos utilizando las herramientas y bibliotecas vistas en clases, como Natural Language Toolkit (NLTK), Stanza y funciones de manipulación de datos de Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A.- Configuración Inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install stanza\n",
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se importó la biblioteca 'string' para trabajar con cadena de texto y manipulación de caracteres.\n",
    "- Se importó la biblioteca 'nltk' (Natural Language Toolkit) para el procesamiento del lenguaje natural.\n",
    "- Se importó 'Stanza' para el procesamiento avanzado del lenguaje natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración de Stanza y Natural Language ToolKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ferna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se descargó el parquete 'punkt' de NLTK para la tokenización de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a540725b22421e847314e3b6d93b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 00:10:19 INFO: Downloaded file to C:\\Users\\ferna\\stanza_resources\\resources.json\n",
      "2024-07-10 00:10:19 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2024-07-10 00:10:20 INFO: File exists: C:\\Users\\ferna\\stanza_resources\\es\\default.zip\n",
      "2024-07-10 00:10:44 INFO: Finished downloading models and saved to C:\\Users\\ferna\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "# Configurar Stanza\n",
    "stanza.download('es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se descargó el modelo en español para Stanza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **B.- Planificación y Análisis Inicial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función recibirá un texto como entrada y realizará varias transformaciones para corregir el uso de elipsis. En el string corregido no se eliminarán todas las elipsis, pero si las señaladas en las instrucciones de esta tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flujo de trabajo proyectado:\n",
    "1. Tokenización del texto en oraciones.\n",
    "2.  Procesamiento de oraciones con Stanza.\n",
    "3.  Corrección de elipsis.\n",
    "    - Repetición de palabras o bigramas.\n",
    "    - Ajuste de artículos.\n",
    "    - Patrones gramaticales.\n",
    "4.  Reconstrucción del texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto de prueba\n",
    "texto_prueba = (\"El acuario es chiquito chiquito. Hay otro más grande pero … fuera del … de la ciudad, y no teníamos ganas de alejarnos mucho. Hay una sección con … con pececitos, cangrejos y bichos por el estilo, todos de la zona. Y otra sección con especies recogidas, ya sea porque los … las abandonaron, o porque están lesionadas y no pueden ser devueltas al ecosistema. Entre ellas había una tortuga … una tortuga de agua, grande, tendría sus 50 cm de … caparazón, y la pobre era cieguita. También había serpientes y lagartos, y uno podía interactuar con ellos. Maddalena dijo “no, gracias”, pero yo dije “siiii” a todo, así que me vi envuelta por serpientes y lagartos. Maddalena me tomó varias fotos con su teléfono, y le di mi correo para que me las mande, así que estoy esperando esas fotos. Salimos del acuario, nos despedimos para siempre con un abrazo, y me volví al … hotel. No tenía mucha idea como volver, pero el … la intuición me llevó derechito y llegué cerca de las 4.00. Como era bien temprano, y no tenía ganas de salir otra vez (hacía muucho calor) me puse el traje de baño y bajé a la piscina. Me bañé un ratito, y luego, aprovechando el lindo día, me quedé un rato disfrutando el sol, cuidando de no quedarme dormida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización del texto en oraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El acuario es chiquito chiquito.',\n",
       " 'Hay otro más grande pero … fuera del … de la ciudad, y no teníamos ganas de alejarnos mucho.',\n",
       " 'Hay una sección con … con pececitos, cangrejos y bichos por el estilo, todos de la zona.',\n",
       " 'Y otra sección con especies recogidas, ya sea porque los … las abandonaron, o porque están lesionadas y no pueden ser devueltas al ecosistema.',\n",
       " 'Entre ellas había una tortuga … una tortuga de agua, grande, tendría sus 50 cm de … caparazón, y la pobre era cieguita.',\n",
       " 'También había serpientes y lagartos, y uno podía interactuar con ellos.',\n",
       " 'Maddalena dijo “no, gracias”, pero yo dije “siiii” a todo, así que me vi envuelta por serpientes y lagartos.',\n",
       " 'Maddalena me tomó varias fotos con su teléfono, y le di mi correo para que me las mande, así que estoy esperando esas fotos.',\n",
       " 'Salimos del acuario, nos despedimos para siempre con un abrazo, y me volví al … hotel.',\n",
       " 'No tenía mucha idea como volver, pero el … la intuición me llevó derechito y llegué cerca de las 4.00.',\n",
       " 'Como era bien temprano, y no tenía ganas de salir otra vez (hacía muucho calor) me puse el traje de baño y bajé a la piscina.',\n",
       " 'Me bañé un ratito, y luego, aprovechando el lindo día, me quedé un rato disfrutando el sol, cuidando de no quedarme dormida.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenización del texto en oraciones:\n",
    "oraciones= nltk.sent_tokenize(texto_prueba)\n",
    "oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El texto fue dividido en 12 oraciones.\n"
     ]
    }
   ],
   "source": [
    "print(f'El texto fue dividido en {len(oraciones)} oraciones.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de oraciones con Stanza:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuración del pipeline de procesamiento de Stanza con tokenización, multi-word expansion (mwt), etiquetado gramatical (pos) y lemmatización para el idioma español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 00:10:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2b1676813a4528953053a4f574b1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 00:10:45 INFO: Downloaded file to C:\\Users\\ferna\\stanza_resources\\resources.json\n",
      "2024-07-10 00:10:45 INFO: Loading these models for language: es (Spanish):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-07-10 00:10:45 INFO: Using device: cpu\n",
      "2024-07-10 00:10:45 INFO: Loading: tokenize\n",
      "2024-07-10 00:10:47 INFO: Loading: mwt\n",
      "2024-07-10 00:10:47 INFO: Loading: pos\n",
      "2024-07-10 00:10:47 INFO: Loading: lemma\n",
      "2024-07-10 00:10:47 INFO: Loading: depparse\n",
      "2024-07-10 00:10:48 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Procesamiento de oraciones con Stanza.\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando que para esta tarea se deben abarcar los siguientes casos:\n",
    "1.\tPalabras (y bigramas) repetidas.\n",
    "2.\tAjuste de artículos (definidos e indefinidos, singular y plural).\n",
    "3.\tPatrones gramaticales: hay patrones gramaticales en los que una elipsis al medio podría perfectamente borrarse. (los patrones que debería incluir serían artículo-sustantivo,  sustantivo-adjetivo, preposición-sustantivo).\n",
    "\n",
    "Se utilizará la columna 'tag' del procesamiento de texto natural obtenido con Stanza, ya que esta columna proporciona las etiquetas gramaticales que corresponden a las partes del discurso (Part-of-Speech tags) de cada palabra, permitiendo identificar artículos, adjetivos, adverbios, pronombres y otros tipos de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer información de dependencias:\n",
    "def depen_parser(mytext):\n",
    "   out_parser = nlp(mytext) # Procesa el texto con Stanza\n",
    "   parser_text = [word.text for sent in out_parser.sentences for word in sent.words]\n",
    "   parser_tag = [word.upos for sent in out_parser.sentences for word in sent.words] \n",
    "   dict_pos = {'word': parser_text,'tag':parser_tag} # Se crea un diccionario con la info. extraída.\n",
    "   df_parser = pd.DataFrame(dict_pos) \n",
    "   return df_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word    tag\n",
      "0        El    DET\n",
      "1   acuario   NOUN\n",
      "2        es    AUX\n",
      "3  chiquito    ADJ\n",
      "4  chiquito    ADJ\n",
      "5         .  PUNCT\n",
      "        word    tag\n",
      "0        Hay   VERB\n",
      "1       otro   PRON\n",
      "2        más    ADV\n",
      "3     grande    ADJ\n",
      "4       pero  CCONJ\n",
      "5          …  PUNCT\n",
      "6      fuera    ADV\n",
      "7         de    ADP\n",
      "8         el    DET\n",
      "9          …  PUNCT\n",
      "10        de    ADP\n",
      "11        la    DET\n",
      "12    ciudad   NOUN\n",
      "13         ,  PUNCT\n",
      "14         y  CCONJ\n",
      "15        no    ADV\n",
      "16  teníamos   VERB\n",
      "17     ganas   NOUN\n",
      "18        de    ADP\n",
      "19    alejar   VERB\n",
      "20       nos   PRON\n",
      "21     mucho   PRON\n",
      "22         .  PUNCT\n",
      "         word    tag\n",
      "0         Hay   VERB\n",
      "1         una    DET\n",
      "2     sección   NOUN\n",
      "3         con    ADP\n",
      "4           …  PUNCT\n",
      "5         con    ADP\n",
      "6   pececitos   NOUN\n",
      "7           ,  PUNCT\n",
      "8   cangrejos   NOUN\n",
      "9           y  CCONJ\n",
      "10     bichos   NOUN\n",
      "11        por    ADP\n",
      "12         el    DET\n",
      "13     estilo   NOUN\n",
      "14          ,  PUNCT\n",
      "15      todos   PRON\n",
      "16         de    ADP\n",
      "17         la    DET\n",
      "18       zona   NOUN\n",
      "19          .  PUNCT\n",
      "           word    tag\n",
      "0             Y  CCONJ\n",
      "1          otra    DET\n",
      "2       sección   NOUN\n",
      "3           con    ADP\n",
      "4      especies   NOUN\n",
      "5     recogidas    ADJ\n",
      "6             ,  PUNCT\n",
      "7            ya    ADV\n",
      "8           sea    AUX\n",
      "9        porque  SCONJ\n",
      "10          los    DET\n",
      "11            …  PUNCT\n",
      "12          las   PRON\n",
      "13  abandonaron   VERB\n",
      "14            ,  PUNCT\n",
      "15            o  CCONJ\n",
      "16       porque  SCONJ\n",
      "17        están    AUX\n",
      "18   lesionadas    ADJ\n",
      "19            y  CCONJ\n",
      "20           no    ADV\n",
      "21       pueden    AUX\n",
      "22          ser    AUX\n",
      "23    devueltas   VERB\n",
      "24            a    ADP\n",
      "25           el    DET\n",
      "26   ecosistema   NOUN\n",
      "27            .  PUNCT\n",
      "         word    tag\n",
      "0       Entre    ADP\n",
      "1       ellas   PRON\n",
      "2       había   VERB\n",
      "3         una    DET\n",
      "4     tortuga   NOUN\n",
      "5           …  PUNCT\n",
      "6         una    DET\n",
      "7     tortuga   NOUN\n",
      "8          de    ADP\n",
      "9        agua   NOUN\n",
      "10          ,  PUNCT\n",
      "11     grande    ADJ\n",
      "12          ,  PUNCT\n",
      "13    tendría   VERB\n",
      "14        sus    DET\n",
      "15         50    NUM\n",
      "16         cm   NOUN\n",
      "17         de    ADP\n",
      "18          …  PUNCT\n",
      "19  caparazón   NOUN\n",
      "20          ,  PUNCT\n",
      "21          y  CCONJ\n",
      "22         la    DET\n",
      "23      pobre    ADJ\n",
      "24        era    AUX\n",
      "25   cieguita    ADJ\n",
      "26          .  PUNCT\n",
      "           word    tag\n",
      "0       También    ADV\n",
      "1         había   VERB\n",
      "2    serpientes   NOUN\n",
      "3             y  CCONJ\n",
      "4      lagartos   NOUN\n",
      "5             ,  PUNCT\n",
      "6             y  CCONJ\n",
      "7           uno   PRON\n",
      "8         podía    AUX\n",
      "9   interactuar   VERB\n",
      "10          con    ADP\n",
      "11        ellos   PRON\n",
      "12            .  PUNCT\n",
      "          word    tag\n",
      "0    Maddalena  PROPN\n",
      "1         dijo   VERB\n",
      "2            “  PUNCT\n",
      "3           no    ADV\n",
      "4            ,  PUNCT\n",
      "5      gracias   NOUN\n",
      "6            ”  PUNCT\n",
      "7            ,  PUNCT\n",
      "8         pero  CCONJ\n",
      "9           yo   PRON\n",
      "10        dije   VERB\n",
      "11           “  PUNCT\n",
      "12       siiii   INTJ\n",
      "13           ”  PUNCT\n",
      "14           a    ADP\n",
      "15        todo   PRON\n",
      "16           ,  PUNCT\n",
      "17         así    ADV\n",
      "18         que  SCONJ\n",
      "19          me   PRON\n",
      "20          vi   VERB\n",
      "21    envuelta    ADJ\n",
      "22         por    ADP\n",
      "23  serpientes   NOUN\n",
      "24           y  CCONJ\n",
      "25       lagar   VERB\n",
      "26         tos   NOUN\n",
      "27           .  PUNCT\n",
      "         word    tag\n",
      "0   Maddalena  PROPN\n",
      "1          me   PRON\n",
      "2        tomó   VERB\n",
      "3      varias    DET\n",
      "4       fotos   NOUN\n",
      "5         con    ADP\n",
      "6          su    DET\n",
      "7    teléfono   NOUN\n",
      "8           ,  PUNCT\n",
      "9           y  CCONJ\n",
      "10         le   PRON\n",
      "11         di   VERB\n",
      "12         mi    DET\n",
      "13     correo   NOUN\n",
      "14       para    ADP\n",
      "15        que  SCONJ\n",
      "16         me   PRON\n",
      "17        las   PRON\n",
      "18      mande   VERB\n",
      "19          ,  PUNCT\n",
      "20        así    ADV\n",
      "21        que  SCONJ\n",
      "22      estoy    AUX\n",
      "23  esperando   VERB\n",
      "24       esas    DET\n",
      "25      fotos   NOUN\n",
      "26          .  PUNCT\n",
      "          word    tag\n",
      "0      Salimos   VERB\n",
      "1           de    ADP\n",
      "2           el    DET\n",
      "3      acuario   NOUN\n",
      "4            ,  PUNCT\n",
      "5          nos   PRON\n",
      "6   despedimos   VERB\n",
      "7         para    ADP\n",
      "8      siempre    ADV\n",
      "9          con    ADP\n",
      "10          un    DET\n",
      "11      abrazo   NOUN\n",
      "12           ,  PUNCT\n",
      "13           y  CCONJ\n",
      "14          me   PRON\n",
      "15       volví   VERB\n",
      "16           a    ADP\n",
      "17          el    DET\n",
      "18           …  PUNCT\n",
      "19       hotel   NOUN\n",
      "20           .  PUNCT\n",
      "         word    tag\n",
      "0          No    ADV\n",
      "1       tenía   VERB\n",
      "2       mucha    DET\n",
      "3        idea   NOUN\n",
      "4        como  SCONJ\n",
      "5      volver   VERB\n",
      "6           ,  PUNCT\n",
      "7        pero  CCONJ\n",
      "8          el    DET\n",
      "9           …  PUNCT\n",
      "10         la    DET\n",
      "11  intuición   NOUN\n",
      "12         me   PRON\n",
      "13      llevó   VERB\n",
      "14  derechito    ADJ\n",
      "15          y  CCONJ\n",
      "16     llegué   VERB\n",
      "17      cerca    ADV\n",
      "18         de    ADP\n",
      "19        las    DET\n",
      "20       4.00   NOUN\n",
      "21          .  PUNCT\n",
      "        word    tag\n",
      "0       Como  SCONJ\n",
      "1        era    AUX\n",
      "2       bien    ADV\n",
      "3   temprano    ADV\n",
      "4          ,  PUNCT\n",
      "5          y  CCONJ\n",
      "6         no    ADV\n",
      "7      tenía   VERB\n",
      "8      ganas   NOUN\n",
      "9         de    ADP\n",
      "10     salir   VERB\n",
      "11      otra    DET\n",
      "12       vez   NOUN\n",
      "13         (  PUNCT\n",
      "14     hacía   VERB\n",
      "15    muucho    DET\n",
      "16     calor   NOUN\n",
      "17         )  PUNCT\n",
      "18        me   PRON\n",
      "19      puse   VERB\n",
      "20        el    DET\n",
      "21     traje   NOUN\n",
      "22        de    ADP\n",
      "23      baño   NOUN\n",
      "24         y  CCONJ\n",
      "25      bajé   VERB\n",
      "26         a    ADP\n",
      "27        la    DET\n",
      "28   piscina   NOUN\n",
      "29         .  PUNCT\n",
      "            word    tag\n",
      "0             Me   PRON\n",
      "1           bañé   VERB\n",
      "2             un    DET\n",
      "3         ratito   NOUN\n",
      "4              ,  PUNCT\n",
      "5              y  CCONJ\n",
      "6          luego    ADV\n",
      "7              ,  PUNCT\n",
      "8   aprovechando   VERB\n",
      "9             el    DET\n",
      "10         lindo    ADJ\n",
      "11           día   NOUN\n",
      "12             ,  PUNCT\n",
      "13            me   PRON\n",
      "14         quedé   VERB\n",
      "15            un    DET\n",
      "16          rato   NOUN\n",
      "17   disfrutando   VERB\n",
      "18            el    DET\n",
      "19           sol   NOUN\n",
      "20             ,  PUNCT\n",
      "21      cuidando   VERB\n",
      "22            de    ADP\n",
      "23            no    ADV\n",
      "24        quedar   VERB\n",
      "25            me   PRON\n",
      "26       dormida    ADJ\n",
      "27             .  PUNCT\n"
     ]
    }
   ],
   "source": [
    "for oracion in oraciones:\n",
    "    doc = nlp(oracion)\n",
    "    df_parser = depen_parser(oracion)\n",
    "    \n",
    "    \n",
    "    print(df_parser)\n",
    "    \n",
    "    tokens=df_parser['word'].tolist()\n",
    "    tags=df_parser['tag'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observaciones encontradas:\n",
    "1. Hay algunas palabras que se dividieron en el proceso parser, como por ejemplo: \n",
    "    - \"alejar\" \"nos\" (VERB+PRON), en vez de \"alejarnos\".\n",
    "    - \"quedar\" \"me\" (VERB+NOUN), en vez de \"quedarme\".\n",
    "    - \"lagar\"  \"tos\" (VERB+NOUN), en vez de \"lagartos\".\n",
    "\n",
    "    Estos detalles se deben a cómo Stanza tokeniza y etiqueta las palabras. Para abordar estos casos, se implementará una lógica adicional en la función, para combinar correctamente las palabras separadas que deberían estar unidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Por otra parte, se observa que en la frase *'Y otra sección con especies recogidas, ya sea porque los … las abandonaron, o porque están lesionadas y no pueden ser devueltas al ecosistema.'*, la palabra 'los' es un artículo definido plural 'DET' y la palabra 'las' es un pronombre 'PRON'. En este caso, 'las' está actuando como un pronombre que reemplaza un sustantivo plural femenino mencionado anteriormente, refiriéndose a 'las especies'.\n",
    "\n",
    "    Por lo tanto, dado que Stanza puede etiquetar 'las' como 'PRON' cuando actúa como pronombre, se ajustará la función para considerar tanto 'DET' como 'PRON' para minimizar estos problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideraciones previas a la creación de la función:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizando el resultado del procesamiento con Stanza anterior, se deben tener en cuenta los siguientes aspectos:\n",
    "\n",
    "- Caso 1 \"Palabras y bigramas repetidos\":\n",
    "    - Deben aplicarse a cualquier tipo de POS tagging.\n",
    "    - Eliminar una de ambas.\n",
    "    - Ejemplos en el texto de prueba:\n",
    "        - *\"Hay una sección con … con pececitos\"*\n",
    "        - *\"Entre ellas había una tortuga … una tortuga de agua, grande\"*\n",
    "\n",
    "- Caso 2 \"Ajuste de artículos (definidos e indefinidos, singular y plural)\":\n",
    "    - Considerando 'las' puede ser un pronombre 'PRON' o artículo definido plural 'DET', para efectos de esta tarea, se ajustará la función tanto para artículos 'DET' como para pronombres 'PRON'.\n",
    "    - Artículos definidos: 'el','la','los','las'.\n",
    "    - Artículos indefinidos: 'un','una','unos','unas'.\n",
    "    - Si el evento que está antes es artículo 'DET' y el evento que está después de la elipsis es artículo 'DET' o pronombre 'PRON', se debe eliminar el primero y conservar el posterior a la elipsis (asumiendo que el hablante se corrigió de forma correcta).\n",
    "    - Ejemplos en el texto de prueba:\n",
    "        - *\"ya sea porque los … las abandonaron\"* \n",
    "        - *\"No tenía mucha idea como volver, pero el … la intuición me llevó derechito y llegué cerca de las 4.00.\"*\n",
    "\n",
    "- Caso 3 \"Patrones gramaticales\":\n",
    "    - artículo-sustantivo: DET-NOUN.\n",
    "    - sustantivo-adjetivo: NOUN-ADJ.\n",
    "    - preposición-sustantivo: ADP-NOUN.\n",
    "    - Ejemplos en el texto de prueba: \n",
    "        - *\"tendría sus 50 cm de … caparazón, y la pobre era cieguita.\"* Este caso corresponde a preposición-sustantivo.\n",
    "        - *\"Salimos del acuario, nos despedimos para siempre con un abrazo, y me volví al … hotel.\"* En este caso corresponde a preposición-sustantivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C.- Definición de la función para corregir elipsis.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def corregir_elipsis(texto):\n",
    "    oraciones = nltk.sent_tokenize(texto)  # Tokeniza el texto en oraciones\n",
    "    \n",
    "    nuevo_texto = []  # Lista para almacenar el texto corregido\n",
    "    \n",
    "    for oracion in oraciones:\n",
    "        df_parser = depen_parser(oracion)  # Analiza la oración usando la función depen_parser\n",
    "        \n",
    "        tokens = df_parser['word'].tolist()  # Extrae las palabras tokenizadas\n",
    "        tags = df_parser['tag'].tolist()  # Extrae las etiquetas POS de las palabras\n",
    "        \n",
    "        nuevo_tokens = []  # Lista para almacenar los tokens corregidos\n",
    "        i = 0  # Índice para recorrer los tokens\n",
    "        while i < len(tokens):\n",
    "            if tokens[i] == '…':  # Verifica si el token es una elipsis\n",
    "                # Caso 1: Repetición de palabras o bigramas\n",
    "                if i > 0 and i < len(tokens) - 1:\n",
    "                    if tokens[i - 1] == tokens[i + 1]:  # Verifica si hay una repetición de palabras\n",
    "                        i += 2  # Salta la elipsis y la palabra repetida\n",
    "                        continue\n",
    "                    elif i > 1 and i < len(tokens) - 2 and tokens[i - 2] + ' ' + tokens[i - 1] == tokens[i + 1] + ' ' + tokens[i + 2]:\n",
    "                        # Verifica si hay una repetición de bigramas\n",
    "                        i += 3  # Salta la elipsis y los bigramas repetidos\n",
    "                        continue\n",
    "                \n",
    "                # Caso 2: Ajuste de artículos (definidos e indefinidos, singular y plural)\n",
    "                if i < len(tokens) - 1 and tags[i + 1] in ['DET','PRON']:  # Verifica si el token después de la elipsis es un artículo o pronombre\n",
    "                    if tags[i - 1] == 'DET':  # Verifica si el token antes de la elipsis es un artículo\n",
    "                        nuevo_tokens.pop()  # Elimina el artículo anterior\n",
    "                        nuevo_tokens.append(tokens[i + 1])  # Agrega el nuevo artículo\n",
    "                        i += 2  # Salta la elipsis y el nuevo artículo\n",
    "                        continue\n",
    "                \n",
    "                # Caso 3: Patrones gramaticales (artículo-sustantivo, sustantivo-adjetivo, preposición-sustantivo)\n",
    "                if i < len(tokens) - 1 and tags[i + 1] in ['NOUN', 'ADJ']:  # Verifica si el token después de la elipsis es un sustantivo o adjetivo\n",
    "                    if tags[i - 1] in ['DET', 'NOUN', 'ADJ', 'ADP']:  # Verifica si el token antes de la elipsis es un artículo, sustantivo, adjetivo o preposición\n",
    "                        i += 1  # Salta la elipsis\n",
    "                        continue\n",
    "            \n",
    "            nuevo_tokens.append(tokens[i])  # Agrega el token actual a la lista de tokens corregidos\n",
    "            i += 1  # Avanza al siguiente token\n",
    "        \n",
    "        # Combinación de palabras separadas incorrectamente\n",
    "        j = 0\n",
    "        while j < len(nuevo_tokens) - 1:\n",
    "            if nuevo_tokens[j] == 'alejar' and nuevo_tokens[j + 1] == 'nos':  # Verifica si hay una combinación incorrecta de \"alejar\" y \"nos\"\n",
    "                nuevo_tokens[j] = 'alejarnos'  # Corrige a \"alejarnos\"\n",
    "                del nuevo_tokens[j + 1]  # Elimina el token incorrecto\n",
    "            if nuevo_tokens[j] == 'lagar' and nuevo_tokens[j + 1] == 'tos':  # Verifica si hay una combinación incorrecta de \"lagar\" y \"tos\"\n",
    "                nuevo_tokens[j] = 'lagartos'  # Corrige a \"alagartos\"\n",
    "                del nuevo_tokens[j + 1]  # Elimina el token incorrecto                \n",
    "            elif nuevo_tokens[j] == 'quedar' and nuevo_tokens[j + 1] == 'me':  # Verifica si hay una combinación incorrecta de \"quedar\" y \"me\"\n",
    "                nuevo_tokens[j] = 'quedarme'  # Corrige a \"quedarme\"\n",
    "                del nuevo_tokens[j + 1]  # Elimina el token incorrecto\n",
    "            else:\n",
    "                j += 1  # Avanza al siguiente par de tokens\n",
    "        \n",
    "        nuevo_texto.extend(nuevo_tokens)  # Agrega los tokens corregidos a la lista de texto corregido\n",
    "        if len(nuevo_texto) > 0 and nuevo_texto[-1] != '.':\n",
    "            nuevo_texto.append('.')  # Añade punto al final de cada oración si no está presente\n",
    "    \n",
    "    # Une las oraciones para formar el texto corregido\n",
    "    texto_corregido = ' '.join(nuevo_texto)\n",
    "    \n",
    "    # Corrige errores comunes de puntuación y espacio\n",
    "    texto_corregido = texto_corregido.replace(' .', '.')\n",
    "    texto_corregido = texto_corregido.replace(' ,', ',')\n",
    "    texto_corregido = texto_corregido.replace(' ;', ';')\n",
    "    texto_corregido = texto_corregido.replace(' :', ':')\n",
    "    texto_corregido = texto_corregido.replace('( ', '(')\n",
    "    texto_corregido = texto_corregido.replace(' )', ')')\n",
    "    texto_corregido = texto_corregido.replace('“ ', '“')\n",
    "    texto_corregido = texto_corregido.replace(' ”', '”')\n",
    "    texto_corregido = texto_corregido.replace('a el', 'al')\n",
    "    texto_corregido = texto_corregido.replace('de el', 'del')\n",
    "    \n",
    "    return texto_corregido  # Devuelve el texto corregido\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flujo de trabajo realizado:\n",
    "1. **Tokenización de Oraciones**: La función comienza dividiendo el texto en oraciones usando 'nltk.sent_tokenize'.\n",
    "2. **Procesamiento de Oraciones**: Cada oración se procesa usando 'depen_parser' para obtener los tokens y sus etiquetas POS.\n",
    "3. **Corrección de Elipsis**:\n",
    "    - Caso 1: Elimina las repeticiones de palabras y bigramas.\n",
    "    - Caso 2: Ajusta los artículos antes y después de la elipsis.\n",
    "    - Caso 3: Corrige patrones gramaticales específicos.\n",
    "4. **Combinación de Palabras**: Corrige palabras que se han dividido incorrectamente en el proceso de tokenización.\n",
    "5. **Unión y Correción del Texto**: Une los tokens corregidos en un solo texto y corrige errores comunes de puntuación y espacio.\n",
    "6. **Devolución del Texto**: Devuelve el texto corregido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **D. Prueba de la función**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto Original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El acuario es chiquito chiquito. Hay otro más grande pero … fuera del … de la ciudad, y no teníamos ganas de alejarnos mucho. Hay una sección con … con pececitos, cangrejos y bichos por el estilo, todos de la zona. Y otra sección con especies recogidas, ya sea porque los … las abandonaron, o porque están lesionadas y no pueden ser devueltas al ecosistema. Entre ellas había una tortuga … una tortuga de agua, grande, tendría sus 50 cm de … caparazón, y la pobre era cieguita. También había serpientes y lagartos, y uno podía interactuar con ellos. Maddalena dijo “no, gracias”, pero yo dije “siiii” a todo, así que me vi envuelta por serpientes y lagartos. Maddalena me tomó varias fotos con su teléfono, y le di mi correo para que me las mande, así que estoy esperando esas fotos. Salimos del acuario, nos despedimos para siempre con un abrazo, y me volví al … hotel. No tenía mucha idea como volver, pero el … la intuición me llevó derechito y llegué cerca de las 4.00. Como era bien temprano, y no tenía ganas de salir otra vez (hacía muucho calor) me puse el traje de baño y bajé a la piscina. Me bañé un ratito, y luego, aprovechando el lindo día, me quedé un rato disfrutando el sol, cuidando de no quedarme dormida.\n"
     ]
    }
   ],
   "source": [
    "print(texto_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto Corregido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El acuario es chiquito chiquito. Hay otro más grande pero … fuera del … de la ciudad, y no teníamos ganas de alejarnos mucho. Hay una sección con pececitos, cangrejos y bichos por el estilo, todos de la zona. Y otra sección con especies recogidas, ya sea porque las abandonaron, o porque están lesionadas y no pueden ser devueltas al ecosistema. Entre ellas había una tortuga de agua, grande, tendría sus 50 cm de caparazón, y la pobre era cieguita. También había serpientes y lagartos, y uno podía interactuar con ellos. Maddalena dijo “no, gracias”, pero yo dije “siiii” a todo, así que me vi envuelta por serpientes y lagartos. Maddalena me tomó varias fotos con su teléfono, y le di mi correo para que me las mande, así que estoy esperando esas fotos. Salimos del acuario, nos despedimos para siempre con un abrazo, y me volví al hotel. No tenía mucha idea como volver, pero la intuición me llevó derechito y llegué cerca de las 4.00. Como era bien temprano, y no tenía ganas de salir otra vez (hacía muucho calor) me puse el traje de baño y bajé a la piscina. Me bañé un ratito, y luego, aprovechando el lindo día, me quedé un rato disfrutando el sol, cuidando de no quedarme dormida.\n"
     ]
    }
   ],
   "source": [
    "texto_corregido = corregir_elipsis(texto_prueba)\n",
    "print(texto_corregido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **E. Análisis de Resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función desarrollada recibió un texto como entrada y realizó varias transformaciones para corregir el uso de elipsis. El objetivo era mejorar la coherencia y legibilidad del texto, eliminando interrupciones innecesarias y ajustando estructuras gramaticales específicas.\n",
    "\n",
    "En el string corregido, no se eliminaron todas las elipsis, pero sí aquellas señaladas en las instrucciones de esta tarea. La función procesó  adecuadamente los casos indicados, logrando los siguientes resultados:\n",
    "\n",
    "###  Transformaciones Realizadas y Resultados Observados:\n",
    "\n",
    "1. Corrección de palabras y bigramas con repeticiones:\n",
    "    - La función identificó y eliminó repeticiones de palabras y bigramas.\n",
    "    - Ejemplo observado:\n",
    "        - Original: *\"Hay una sección con ... con pececitos, cangrejos y bichos por el estilo, todos de la zona.\"*\n",
    "        - Corregido: *\"Hay una sección con pececitos, cangrejos y bichos por el estilo, todos de la zona.\"*\n",
    "\n",
    "2. Ajuste de Artículos:\n",
    "    - Se ajustaron artículos definidos e indefinidos, singular y plural, manteniendo solo el artículo posteiror a la elipsis. Por ejemplo, *\"el ... la \"* se corrigió a *\"la\"*.\n",
    "    - Ejemplo observado N°1: \n",
    "        - Original: *\"ya sea porque los ... las abandonaron\"*\n",
    "        - Corregido: *\"ya sea porque las abandonaron\"*\n",
    "    - Ejemplo observado N°2: \n",
    "        - Original: *\"pero el ... la intuición me llevó\"*\n",
    "        - Corregido: *\"pero la intuición me llevó\"*\n",
    "\n",
    "3. Patrones Gramaticales:\n",
    "    - Se corrigieron  patrones gramaticales como artículo-sustantivo, sustantivo-adjetivo y preposición-sustantivo.\n",
    "    - Ejemplo observado:\n",
    "        - Original: *\"Entre ellas había una tortuga ... una tortuga de agua, grande, tendría sus 50  cm de ... caparazón\"*\n",
    "        - Corregido: *\"Entre ellas había una tortuga de agua, grande, tendría sus 50 cm de caparazón\"*\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **III.- Conclusión**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta tarea, se ha desarrollado un enfoque sistemático para el procesamiento de textos con el objetivo de corregir el uso de elipsis en transcripciones literales típicas de entrevistas y focus groups. Utilizando herramientas y bibliotecas vistas en clases como NLTK y Stanza, se implementó una función que normaliza el texto, elimina puntuación, tokeniza palabras y corrige elipsis basándose en las reglas específicas del enunciado.\n",
    "\n",
    "El proceso de eliminar interrupciones innecesarias y ajustar estructuras gramaticales permitió mejorar significativamente la coherencia y legibilidad del texto. Esto puede facilitar posteriormente el análisis en aplicaciones de aprendizaje automático, procesamiento de lenguaje natural (NLP) y análisis linguistico. \n",
    "\n",
    "Durante el desarrollo, se identificaron algunos detalles importantes. Por ejemplo, algunas palabras se dividieron incorrectamente en el proceso de análisis, como *\"alejar\"* *\"nos\"* (VERB+PRON) en lugar de *\"alejarnos\"* y *\"quedar\"* *\"me\"* (VERB+PRON) en lugar de *\"quedarme\"*. Por lo tanto, para abordar estos casos, se implementó una lógica adicional en la función para combinar correctamente las palabras separadas que deberían estar unidas.\n",
    "\n",
    "Finalmente, se observó que en la frase *\"y otra sección con especies recogidas, ya sea porque los ... las abandonaron, o porque están lesionadas y no pueden ser devueltas al ecosistema,\"* la palabra *\"los\"* es un artículo definido plural (DET) y la palabra *\"las\"* es un pronombre (PRON). En este contexto, *\"las\"* actúa como un pronombre que reemplaza un sustantivo plural femenino mencionado anteriormente, refiriéndose a \"las especies\". Por lo tanto, se ajustó la función para considerar tanto 'DET' como 'PRON' para manejar estos casos de manera adecuada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
